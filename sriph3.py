# -*- coding: utf-8 -*-
"""sriph3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17EJR4euIdLZ73ZALKz-tm4juG0DdkJv6
"""

from google.colab import files
uploaded = files.upload()

import pandas as pd

# Read the dataset
df = pd.read_csv('road_accident_dataset.csv')

# Display first few rows
df.head()

# Shape of the dataset
print("Shape:", df.shape)

# Column names
print("Columns:", df.columns.tolist())

# Data types and non-null values
df.info()

# Summary statistics for numeric features
df.describe()

# Check for missing values
print(df.isnull().sum())

# Check for duplicates
print("Duplicate rows:", df.duplicated().sum())

import seaborn as sns
import matplotlib.pyplot as plt

# Check available columns (optional, for verification)
print(df.columns.tolist())

# Correct plot using the actual column name from your dataset
sns.countplot(x='Accident Severity', data=df)
plt.title('Accident Severity Distribution')
plt.show()

target = 'Accident Severity'
features = df.columns.drop(target)
print("Features:", features)

# Identify categorical columns
categorical_cols = df.select_dtypes(include=['object']).columns
print("Categorical Columns:", categorical_cols.tolist())

!pip install gradio

import gradio as gr
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
import joblib
import os # Import os module

# Assuming you have already loaded and preprocessed your data into a DataFrame 'df'
# and defined your target and features.
# If not, ensure the previous cells for loading and basic exploration are run.

# Define target and features (as done in previous cells)
target = 'Accident Severity'
features = df.columns.drop(target)

# Separate target variable
X = df[features]
y = df[target]

# Identify categorical and numerical columns
categorical_cols = X.select_dtypes(include=['object']).columns
numerical_cols = X.select_dtypes(include=np.number).columns

# Create preprocessing pipelines for numerical and categorical features
# Use 'passthrough' for numerical columns to keep them as is (only scaling later)
# Use OneHotEncoder for categorical columns
preprocessor = ColumnTransformer(
    transformers=[
        ('num', 'passthrough', numerical_cols),
        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)
    ])

# Create a full pipeline including preprocessing, scaling, and the model
model_pipeline = Pipeline(steps=[('preprocessor', preprocessor),
                               ('scaler', StandardScaler(with_mean=False)), # Scale after one-hot encoding
                               ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))])

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Train the model
model_pipeline.fit(X_train, y_train)

# Save the trained model
joblib.dump(model_pipeline['classifier'], "accident_severity_model.pkl")

# Save the trained scaler separately (as we are using it within the pipeline)
# A more robust approach is to save the entire pipeline, but for matching the original code structure:
# You'd typically save the scaler trained on the scaled data *after* fitting the pipeline preprocessor.
# Let's fit a separate scaler only on the transformed data for clarity based on the original loading code.
# This might be slightly different than scaling *within* the pipeline on all data after one-hot encoding.
# For the original code's structure to work, we need a scaler trained on the final feature space.
# Let's refit a scaler on the training data after preprocessing and then save it.
X_train_processed = model_pipeline['preprocessor'].transform(X_train)
scaler_separate = StandardScaler(with_mean=False)
scaler_separate.fit(X_train_processed)
joblib.dump(scaler_separate, "scaler.pkl")

# Get the list of columns after preprocessing (including one-hot encoded columns)
# This requires fitting the preprocessor first to get the feature names
preprocessor.fit(X_train) # Refit preprocessor to get feature names after transforming
model_columns = preprocessor.get_feature_names_out(input_features=X_train.columns)
joblib.dump(model_columns, "model_columns.pkl")

print("Model, scaler, and column list saved successfully.")

# Now, load your trained model and scaler (this part is the same as your original code)
# Check if files exist before attempting to load
if os.path.exists("accident_severity_model.pkl") and os.path.exists("scaler.pkl") and os.path.exists("model_columns.pkl"):
    model = joblib.load("accident_severity_model.pkl")         # Trained RandomForestClassifier
    scaler = joblib.load("scaler.pkl")                          # Trained StandardScaler
    model_columns = joblib.load("model_columns.pkl")          # List of columns used during training
    print("Model, scaler, and column list loaded successfully.")
else:
    print("Error: Model or scaler files not found. Please ensure they are trained and saved.")


# Define prediction function
def predict_severity(time_of_day, road_type, weather, light_condition, vehicle_count, speed_limit):
    # Create a DataFrame from inputs with the exact column names expected by your model
    # Ensure these column names match the features used during training *before* encoding
    input_data = pd.DataFrame([{
        'Time_of_Day': time_of_day, # Use the actual column names from your dataset
        'Road_Type': road_type,
        'Weather_Condition': weather,
        'Light_Condition': light_condition,
        'Number_of_Vehicles': int(vehicle_count), # Use the actual column names from your dataset
        'Speed_Limit': int(speed_limit) # Use the actual column names from your dataset
    }])

    # Reapply the same preprocessing steps as during training
    # Use the trained preprocessor from the pipeline to transform the new data
    input_processed = model_pipeline['preprocessor'].transform(input_data)

    # Scale using the separate scaler that was trained on the processed training data
    scaled_input = scaler.transform(input_processed)

    # Predict
    # Ensure the model loaded (model = joblib.load(...)) is used
    if 'model' in globals() and model is not None:
        pred = model.predict(scaled_input)[0]
        # Map the numerical prediction to the corresponding label
        # Assuming the target variable was encoded to 0, 1, 2 for Slight, Serious, Fatal
        # You might need to adjust this mapping based on how your target was encoded
        severity_mapping = {0: "Slight", 1: "Serious", 2: "Fatal"}
        pred_label = severity_mapping.get(pred, "Unknown") # Handle potential unknown predictions

        return f"üö® Predicted Accident Severity: {pred_label}"
    else:
        return "Error: Model not loaded. Cannot make prediction."


# Gradio Interface
inputs = [
    # Ensure dropdown options exactly match the categories in your training data
    gr.Dropdown(list(df['Time_of_Day'].unique()), label="Time of Day"), # Use unique values from your data
    gr.Dropdown(list(df['Road_Type'].unique()), label="Road Type"),
    gr.Dropdown(list(df['Weather_Condition'].unique()), label="Weather Condition"),
    gr.Dropdown(list(df['Light_Condition'].unique()), label="Light Condition"),
    gr.Number(label="Number of Vehicles Involved"),
    gr.Number(label="Speed Limit (km/h)")
]

output = gr.Textbox(label="Prediction")

if 'model' in globals() and model is not None:
    gr.Interface(
        fn=predict_severity,
        inputs=inputs,
        outputs=output,
        title="üõ£Ô∏è Traffic Accident Severity Predictor",
        description="Predicts severity (Slight / Serious / Fatal) based on accident conditions."
    ).launch()
else:
    print("Gradio interface not launched because the model was not loaded.")